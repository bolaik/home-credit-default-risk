{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Baseline Model with NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:14:02.414157Z",
     "start_time": "2018-06-11T07:14:02.396878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from wordcloud import WordCloud\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.plotly as py\n",
    "from plotly import tools\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import random \n",
    "import math\n",
    "import warnings\n",
    "import time\n",
    "import sys\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "pd.set_option(\"display.max_rows\",1001)\n",
    "pd.set_option(\"display.max_columns\",1001)\n",
    "\n",
    "path = \"./input/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:14:02.943493Z",
     "start_time": "2018-06-11T07:14:02.939033Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timer(start,end):\n",
    "    m, s = divmod(end-start, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    print(\"time elapsed: %d:%02d:%02d\" % (h, m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:14:58.712551Z",
     "start_time": "2018-06-11T07:14:05.115290Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_train = pd.read_csv(path + \"application_train.csv\")\n",
    "bureau = pd.read_csv(path + \"bureau.csv\")\n",
    "bureau_balance = pd.read_csv(path + \"bureau_balance.csv\")\n",
    "credit_card_balance = pd.read_csv(path + \"credit_card_balance.csv\")\n",
    "pcb = pd.read_csv(path + \"POS_CASH_balance.csv\")\n",
    "previous_application = pd.read_csv(path + \"previous_application.csv\")\n",
    "installments_payments = pd.read_csv(path + \"installments_payments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:05:29.575232Z",
     "start_time": "2018-06-11T07:05:27.931235Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "import lightgbm as lgb\n",
    "\n",
    "# read the test files \n",
    "app_test = pd.read_csv(path + 'application_test.csv')\n",
    "\n",
    "app_test['is_train'] = 0\n",
    "app_train['is_train'] = 1\n",
    "\n",
    "# target variable\n",
    "Y_train = app_train['TARGET']\n",
    "trainX = app_train.drop(['TARGET'], axis = 1)\n",
    "\n",
    "# test ID\n",
    "test_id = app_test['SK_ID_CURR']\n",
    "testX = app_test\n",
    "\n",
    "# merge train and test datasets for preprocessing\n",
    "data = pd.concat([trainX, testX], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handelling Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:05:52.927977Z",
     "start_time": "2018-06-11T07:05:29.577085Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to obtain Categorical Features\n",
    "def _get_categorical_features(df):\n",
    "    feats = [col for col in list(df.columns) if df[col].dtype == 'object']\n",
    "    return feats\n",
    "\n",
    "# function to factorize categorical features\n",
    "def _factorize_categoricals(df, cats):\n",
    "    for col in cats:\n",
    "        df[col], _ = pd.factorize(df[col])\n",
    "    return df \n",
    "\n",
    "# function to create dummy variables of categorical features\n",
    "def _get_dummies(df, cats):\n",
    "    for col in cats:\n",
    "        df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n",
    "    return df \n",
    "\n",
    "# get categorical features\n",
    "data_cats = _get_categorical_features(data)\n",
    "prev_app_cats = _get_categorical_features(previous_application)\n",
    "bureau_cats = _get_categorical_features(bureau)\n",
    "pcb_cats = _get_categorical_features(pcb)\n",
    "ccbal_cats = _get_categorical_features(credit_card_balance)\n",
    "\n",
    "# create additional dummy features - \n",
    "previous_application = _get_dummies(previous_application, prev_app_cats)\n",
    "bureau = _get_dummies(bureau, bureau_cats)\n",
    "pcb = _get_dummies(pcb, pcb_cats)\n",
    "credit_card_balance = _get_dummies(credit_card_balance, ccbal_cats)\n",
    "\n",
    "# factorize the categorical features from train and test data\n",
    "data = _factorize_categoricals(data, data_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "#### Feature Engineering - Previous Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:06:08.798769Z",
     "start_time": "2018-06-11T07:05:52.929770Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## More Feature Ideas Reference : https://www.kaggle.com/ogrellier/good-fun-with-ligthgbm \n",
    "\n",
    "## count the number of previous applications for a given ID\n",
    "prev_apps_count = previous_application[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "previous_application['SK_ID_PREV'] = previous_application['SK_ID_CURR'].map(prev_apps_count['SK_ID_PREV'])\n",
    "\n",
    "## Average values for all other features in previous applications\n",
    "prev_apps_avg = previous_application.groupby('SK_ID_CURR').mean()\n",
    "prev_apps_avg.columns = ['p_' + col for col in prev_apps_avg.columns]\n",
    "data = data.merge(right=prev_apps_avg.reset_index(), how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering - Bureau Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:06:11.749783Z",
     "start_time": "2018-06-11T07:06:08.800823Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Average Values for all bureau features \n",
    "bureau_avg = bureau.groupby('SK_ID_CURR').mean()\n",
    "bureau_avg['buro_count'] = bureau[['SK_ID_BUREAU','SK_ID_CURR']].groupby('SK_ID_CURR').count()['SK_ID_BUREAU']\n",
    "bureau_avg.columns = ['b_' + f_ for f_ in bureau_avg.columns]\n",
    "data = data.merge(right=bureau_avg.reset_index(), how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering - Previous Installments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:06:17.211491Z",
     "start_time": "2018-06-11T07:06:11.752003Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## count the number of previous installments\n",
    "cnt_inst = installments_payments[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "installments_payments['SK_ID_PREV'] = installments_payments['SK_ID_CURR'].map(cnt_inst['SK_ID_PREV'])\n",
    "\n",
    "## Average values for all other variables in installments payments\n",
    "avg_inst = installments_payments.groupby('SK_ID_CURR').mean()\n",
    "avg_inst.columns = ['i_' + f_ for f_ in avg_inst.columns]\n",
    "data = data.merge(right=avg_inst.reset_index(), how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering - Pos Cash Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:06:24.961521Z",
     "start_time": "2018-06-11T07:06:17.213305Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### count the number of pos cash for a given ID\n",
    "pcb_count = pcb[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "pcb['SK_ID_PREV'] = pcb['SK_ID_CURR'].map(pcb_count['SK_ID_PREV'])\n",
    "\n",
    "## Average Values for all other variables in pos cash\n",
    "pcb_avg = pcb.groupby('SK_ID_CURR').mean()\n",
    "data = data.merge(right=pcb_avg.reset_index(), how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering - Credit Card Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:06:29.793287Z",
     "start_time": "2018-06-11T07:06:24.963379Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### count the number of previous applications for a given ID\n",
    "nb_prevs = credit_card_balance[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\n",
    "credit_card_balance['SK_ID_PREV'] = credit_card_balance['SK_ID_CURR'].map(nb_prevs['SK_ID_PREV'])\n",
    "\n",
    "### average of all other columns \n",
    "avg_cc_bal = credit_card_balance.groupby('SK_ID_CURR').mean()\n",
    "avg_cc_bal.columns = ['cc_bal_' + f_ for f_ in avg_cc_bal.columns]\n",
    "data = data.merge(right=avg_cc_bal.reset_index(), how='left', on='SK_ID_CURR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Final Train and Test data\n",
    "\n",
    "#### Split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:06:33.211158Z",
     "start_time": "2018-06-11T07:06:29.795264Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### prepare final Train X and Test X dataframes \n",
    "ignore_features = ['SK_ID_CURR', 'is_train']\n",
    "relevant_features = [col for col in data.columns if col not in ignore_features]\n",
    "trainX = data[data['is_train'] == 1][relevant_features]\n",
    "testX = data[data['is_train'] == 0][relevant_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Train and Test Data : Impute None and Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:06:33.359919Z",
     "start_time": "2018-06-11T07:06:33.212806Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _preprocess(dtrain, dtest):\n",
    "    print('Start Preprocessing', end='\\t')\n",
    "    bgn_time = time.time()\n",
    "\n",
    "    # replace np.inf to np.nan\n",
    "    dtrain = dtrain.replace([np.inf, -np.inf], np.nan)\n",
    "    dtest = dtest.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # impute np.nan\n",
    "    dtrain_col_mean = dtrain.mean(axis=0)\n",
    "    dtrain, dtest = dtrain.fillna(dtrain_col_mean), dtest.fillna(dtrain_col_mean)\n",
    "\n",
    "    # perform standardization\n",
    "    dtrain_col_mean, dtrain_col_std = dtrain.mean(axis=0), dtrain.std(axis=0)\n",
    "    dtrain, dtest = map(lambda x: (x - dtrain_col_mean) / dtrain_col_std, (dtrain, dtest))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    timer(bgn_time, end_time)\n",
    "\n",
    "    return dtrain, dtest\n",
    "\n",
    "def _preprocess_log(dtrain, dtest):\n",
    "    print('Start Preprocessing with Log Transformation', end='\\t')\n",
    "    bgn_time = time.time()\n",
    "\n",
    "    # replace np.inf to np.nan\n",
    "    dtrain = dtrain.replace([np.inf, -np.inf], np.nan)\n",
    "    dtest = dtest.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # impute np.nan\n",
    "    dtrain_col_mean = dtrain.mean(axis=0)\n",
    "    dtrain, dtest = dtrain.fillna(dtrain_col_mean), dtest.fillna(dtrain_col_mean)\n",
    "\n",
    "    # log transform of min-zero columns\n",
    "    dtrain_col_min = dtrain.min(axis=0)\n",
    "    zero_min_index = dtrain_col_min[dtrain_col_min >= 0].index\n",
    "\n",
    "    dtrain[zero_min_index] = np.log10(dtrain[zero_min_index] + 1.0)\n",
    "    dtest[zero_min_index] = np.log10(dtest[zero_min_index] + 1.0)\n",
    "\n",
    "    # perform standardization\n",
    "    dtrain_col_mean, dtrain_col_std = dtrain.mean(axis=0), dtrain.std(axis=0)\n",
    "    dtrain, dtest = map(lambda x: (x - dtrain_col_mean) / dtrain_col_std, (dtrain, dtest))\n",
    "\n",
    "    end_time = time.time()\n",
    "    timer(bgn_time, end_time)\n",
    "\n",
    "    return dtrain, dtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most of the features are composed of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:06:33.370100Z",
     "start_time": "2018-06-11T07:06:33.361516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356255, 372)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:06:35.593839Z",
     "start_time": "2018-06-11T07:06:33.371959Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(data.isnull().sum() > data.shape[1]*0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:06:44.591283Z",
     "start_time": "2018-06-11T07:06:35.595403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Preprocessing\ttime elapsed: 0:00:08\n"
     ]
    }
   ],
   "source": [
    "# preprocessing with log transformation\n",
    "# X_train, X_test = _preprocess_log(trainX, testX)\n",
    "\n",
    "# preprocessing \n",
    "X_train, X_test = _preprocess(trainX, testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column `p_NAME_GOODS_CATEGORY_House Construction` all None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:26:19.580768Z",
     "start_time": "2018-06-11T07:26:18.749372Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.drop(['p_NAME_GOODS_CATEGORY_House Construction'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Layers of NN model\n",
    "\n",
    "#### Build model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:26:39.411131Z",
     "start_time": "2018-06-11T07:26:36.895484Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras import regularizers\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:26:41.040501Z",
     "start_time": "2018-06-11T07:26:41.030857Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_create_model(x, reg):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=x.shape[1], activation='relu', kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(reg)))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[metrics.binary_accuracy])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:26:44.109647Z",
     "start_time": "2018-06-11T07:26:44.089134Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_run_model(model, dtrain, dtest, batch_size=64, nb_epochs=20, patience=5):\n",
    "    if dtest:\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=patience, verbose=0, mode='auto')\n",
    "        model.fit(dtrain[0], dtrain[1], batch_size=batch_size, epochs=nb_epochs,\n",
    "                  callbacks=[early_stop], validation_data=dtest, verbose=2)\n",
    "        y_train_pred, y_test_pred = model.predict(dtrain[0]), model.predict(dtest[0])\n",
    "        y_train_loss, y_test_loss = log_loss(dtrain[1], y_train_pred), log_loss(dtest[1], y_test_pred)        \n",
    "        y_train_auc, y_test_auc = roc_auc_score(dtrain[1], y_train_pred), roc_auc_score(dtest[1], y_test_pred)        \n",
    "        return model, y_train_loss, y_test_loss, y_train_auc, y_test_auc\n",
    "    else:\n",
    "        model.fit(dtrain[0], dtrain[1], batch_size=batch_size, epochs=nb_epochs, verbose=2)\n",
    "        y_train_pred = model.predict(dtrain[0])\n",
    "        y_train_loss = log_loss(dtrain[1], y_train_pred)\n",
    "        return model, y_train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model with train-validation-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:26:46.659887Z",
     "start_time": "2018-06-11T07:26:46.645863Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_train_val_split(preprocess='linear', reg=0.01, batch_size=256, nb_epochs=50, patience=5):\n",
    "    \n",
    "    train_x, val_x, train_y, val_y = train_test_split(X_train, Y_train, test_size=0.2, random_state=18)\n",
    "    \n",
    "    bgn_time = time.time()\n",
    "        \n",
    "    clf = nn_create_model(train_x, reg)\n",
    "    clf, train_loss, val_loss, train_auc, val_auc = nn_run_model(clf, (train_x, train_y), (val_x, val_y), batch_size, nb_epochs, patience)\n",
    "        \n",
    "    print(\"train_loss: {0:.6f}, val_loss: {1:.6f}, train_auc: {2:.6f}, val_auc:{3:.6f}\".format(train_loss, val_loss, train_auc, val_auc), end=\"\\t\")\n",
    "        \n",
    "    end_time = time.time()\n",
    "    timer(bgn_time, end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-11T07:28:51.083336Z",
     "start_time": "2018-06-11T07:26:53.921925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 246008 samples, validate on 61503 samples\n",
      "Epoch 1/50\n",
      " - 6s - loss: 0.4482 - binary_accuracy: 0.9188 - val_loss: 0.2742 - val_binary_accuracy: 0.9200\n",
      "Epoch 2/50\n",
      " - 6s - loss: 0.2740 - binary_accuracy: 0.9191 - val_loss: 0.2721 - val_binary_accuracy: 0.9200\n",
      "Epoch 3/50\n",
      " - 6s - loss: 0.2737 - binary_accuracy: 0.9191 - val_loss: 0.2732 - val_binary_accuracy: 0.9200\n",
      "Epoch 4/50\n",
      " - 6s - loss: 0.2736 - binary_accuracy: 0.9191 - val_loss: 0.2728 - val_binary_accuracy: 0.9200\n",
      "Epoch 5/50\n",
      " - 6s - loss: 0.2733 - binary_accuracy: 0.9191 - val_loss: 0.2716 - val_binary_accuracy: 0.9200\n",
      "Epoch 6/50\n",
      " - 6s - loss: 0.2728 - binary_accuracy: 0.9191 - val_loss: 0.2706 - val_binary_accuracy: 0.9200\n",
      "Epoch 7/50\n",
      " - 6s - loss: 0.2723 - binary_accuracy: 0.9191 - val_loss: 0.2700 - val_binary_accuracy: 0.9200\n",
      "Epoch 8/50\n",
      " - 6s - loss: 0.2720 - binary_accuracy: 0.9191 - val_loss: 0.2708 - val_binary_accuracy: 0.9200\n",
      "Epoch 9/50\n",
      " - 6s - loss: 0.2720 - binary_accuracy: 0.9191 - val_loss: 0.2692 - val_binary_accuracy: 0.9200\n",
      "Epoch 10/50\n",
      " - 6s - loss: 0.2715 - binary_accuracy: 0.9191 - val_loss: 0.2700 - val_binary_accuracy: 0.9200\n",
      "Epoch 11/50\n",
      " - 6s - loss: 0.2715 - binary_accuracy: 0.9191 - val_loss: 0.2693 - val_binary_accuracy: 0.9200\n",
      "Epoch 12/50\n",
      " - 6s - loss: 0.2712 - binary_accuracy: 0.9191 - val_loss: 0.2693 - val_binary_accuracy: 0.9200\n",
      "Epoch 13/50\n",
      " - 7s - loss: 0.2713 - binary_accuracy: 0.9191 - val_loss: 0.2686 - val_binary_accuracy: 0.9200\n",
      "Epoch 14/50\n",
      " - 6s - loss: 0.2710 - binary_accuracy: 0.9191 - val_loss: 0.2687 - val_binary_accuracy: 0.9200\n",
      "Epoch 15/50\n",
      " - 6s - loss: 0.2710 - binary_accuracy: 0.9191 - val_loss: 0.2692 - val_binary_accuracy: 0.9200\n",
      "Epoch 16/50\n",
      " - 6s - loss: 0.2708 - binary_accuracy: 0.9191 - val_loss: 0.2693 - val_binary_accuracy: 0.9200\n",
      "Epoch 17/50\n",
      " - 6s - loss: 0.2708 - binary_accuracy: 0.9191 - val_loss: 0.2695 - val_binary_accuracy: 0.9200\n",
      "Epoch 18/50\n",
      " - 6s - loss: 0.2707 - binary_accuracy: 0.9191 - val_loss: 0.2691 - val_binary_accuracy: 0.9200\n",
      "train_loss: 0.253627, val_loss: 0.252493, train_auc: 0.746738, val_loss:0.744808\ttime elapsed: 0:01:55\n"
     ]
    }
   ],
   "source": [
    "nn_train_val_split(preprocess='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
